{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/full-colour-logo-UoB.png\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "\n",
    "# Introduction to Programming for Engineers\n",
    "\n",
    "## Python 3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 09 Machine Learing Part 1  \n",
    "## CLASS MATERIAL\n",
    "<br> <a href='#WhyMachineLearning?'>1. Why Machine Learning?</a>\n",
    "<br> <a href='#MachineLearningPython'>2. Machine Learning with Python</a> \n",
    "<br> <a href='#FirstMachineLearningApplication'>3. First Machine Learning Application</a> \n",
    "<br> <a href='#ImportingData'>4. Importing Data</a> \n",
    "<br><a href='#ReviewExercises'>5. Review Exercises</a>\n",
    "\n",
    "<!--BOOK_INFORMATION-->\n",
    "This notebook contains both adapted and unmodified material from: \n",
    "[Introduction to Machine Learning with Python](https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/)\n",
    "by Sarah Guido, Andreas C. Müller; the content is available [on GitHub](https://github.com/amueller/introduction_to_ml_with_python/blob/master/02-supervised-learning.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Science\n",
    "A field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from structured and unstructured data.\n",
    "\n",
    "It employs techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, and informatics. \n",
    "\n",
    "Data sets are modelled and curated to find patterns and make predictions about the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine Learning \n",
    "\n",
    "In Machine learning, algorithms acquire the knowledge or skill through experience. \n",
    "\n",
    "Machine learning relies on big data sets to identify patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Artificial Intelligence (AI)\n",
    "<br>AI is the study of enabling machines to make decisions independently without the need for human interference. \n",
    "\n",
    "Therefore AI tends to be used in situations where adapting to new scenarios are important.\n",
    "<br>As this often involves acquiring knowledge and learning to apply it, Machine Learning is a widely used approach for AI. \n",
    "\n",
    "AI has broad application ranging from robotics to text analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/machine_learning_AI.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lesson Goal\n",
    "\n",
    "- An introduction to machine learning \n",
    "- A simple machine learning example:\n",
    "    - Check data is suitable for analysis using machine learning\n",
    "    - Choose and import a model\n",
    "    - Fit the model to the traning data\n",
    "    - Analyse the accuracy of the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fundamental programming concepts\n",
    "\n",
    "- working with `scikit_learn` and `mglearn` packages\n",
    "- develpoing models using Pandas `DataFrames`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Machine learning methods have in recent years become extremely widely used in everyday life:\n",
    "- automatic recommendations of which movies to watch\n",
    "- what food to order\n",
    "- which products to buy\n",
    "- personalized online music\n",
    "- recognizing your friends in your photos\n",
    "\n",
    "Many modern websites and devices have machine learning algorithms at their core. \n",
    "\n",
    "Every part of a complex website like Facebook, Amazon, or Netflix contains multiple machine learning models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Machine learning has also influenced on the way data-driven research is done today and applied to diverse scientific problems:\n",
    "- understanding stars\n",
    "- finding distant planets\n",
    "- discovering new particles\n",
    "- analyzing DNA sequences\n",
    "- providing personalized cancer treatments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='WhyMachineLearning?'></a>\n",
    "# 1. Why Machine Learning?\n",
    "\n",
    "<br> <a href='#ProblemsMachineLearningCanSolve'>1.1. Problems Machine Learning Can Solve</a>\n",
    "<br> <a href='#SupervisedLearningAlgorithms'>1.2 Supervised Learning Algorithms</a> \n",
    "<br> <a href='#UnsupervisedLearningAlgorithms'>1.3 Unsupervised Learning Algorithms</a> \n",
    "\n",
    "\n",
    "\n",
    "Early “intelligent” applications, used handcoded rules of “if” and “else” decisions, just like the ones you have leanrt to use, to process data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Example : \"Hand-coded\" sorting algorithm__\n",
    "<br>Decide what type of sport an althlete plays based on their weight and height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>In the plot below, each of the circles represents the mean height and weight of athletes grouped by type. \n",
    "<br>The square represents the height and weight of an athlete to be classified. \n",
    "<img src=\"img/vector_quantisation.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "To find the closet point:\n",
    "1. Use broadcasting to find the difference between the x,y cooridinates of the __square__ and the x, y coordinates of each __circle__. <br>\n",
    "1. Find the euclidian distance, $d$ from the square, $s$ to each circle, $c$: <br>$d = \\sqrt{(x_{c}-x_{s})^2 + (y_{c}-y_{s})^2}$ <br>\n",
    "1. Classify athlete as the group corresponding to the minimum distance, $d_{min}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "athletes = ['basketball player',\n",
    "            'football lineman',\n",
    "            'female gymnast',\n",
    "            'marathon runner ']\n",
    "\n",
    "athlete = np.array([111.0,188.0])\n",
    "\n",
    "categories = np.array([[102.0, 203.0],\n",
    "                       [132.0, 193.0],\n",
    "                       [45.0, 155.0],\n",
    "                       [57.0, 173.0]])\n",
    "\n",
    "# 1. broadcast\n",
    "diff = categories - athlete\n",
    "\n",
    "# 2. distance to each point (magnitude of values along axis 1 for each datum)\n",
    "# dist = np.linalg.norm(diff,axis=1)\n",
    "dist = np.sqrt(np.sum(diff**2,axis=1))\n",
    "\n",
    "# 3. which group?\n",
    "nearest = np.argmin(dist)\n",
    "print(athletes[nearest])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The nearest group is index 0 of the array `catagories`.\n",
    "<br>Based on mean height and weight, the athlete is most likely to be a basketball player."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Example : An email spam filter__\n",
    "\n",
    "> Task : move the appropriate incoming email messages to a spam folder. \n",
    "\n",
    "> Solution : Write list of \"spam\" words and sort emails by content using `if` and `else`\n",
    "\n",
    "Using handcoded rules to make decisions has two major disadvantages:\n",
    "- The logic required to make a decision is specific to a single domain and task. Changing the task even slightly might require a rewrite of the whole system.\n",
    "- Designing rules requires a deep understanding of how a decision should be made by a human expert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "spam_folder = []\n",
    "\n",
    "\n",
    "email_subject = pd.Series({'subject' : \"You've won 1 Million dollars!\"})\n",
    "\n",
    "\n",
    "if email_subject.str.contains('Million dollars|MEET SINGLES|Serious cash|No credit check', case=False)[0]:\n",
    "    \n",
    "    spam_folder.append(email_subject)\n",
    "    \n",
    "    \n",
    "spam_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Example of where the hand-coded approach will fail__ : Facial recognition.\n",
    "\n",
    "Facial recognition was an unsolved problem until as recently as 2001.\n",
    "\n",
    "Today every smartphone can detect a face in an image.\n",
    "\n",
    "The way in which pixels (which make up an image in a computer) are “perceived” by the computer is very different from how humans perceive a face. \n",
    "<br>The difference in representation makes it almost impossible for a human to write a set of rules to describe what constitutes a face in a digital image.\n",
    "\n",
    "However, by presenting a program with a large collection of images of faces, an algorithm can determine what characteristics are needed to identify a face.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='ProblemsMachineLearningCanSolve'></a>\n",
    "## 1.1 Problems Machine Learning Can Solve\n",
    "\n",
    "\n",
    "\n",
    "Automated decision-making processes by:\n",
    "- finding patterns in a group of examples.\n",
    "- generalizing from known examples. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalizing from known examples\n",
    "\n",
    "This is known as __supervised learning__.\n",
    "\n",
    "The user provides the algorithm with pairs of inputs and desired outputs.\n",
    "\n",
    "The algorithm finds a way to produce the desired output given an input. \n",
    "\n",
    "In particular, the algorithm is able to create an output for an input it has never seen before without any help from a human. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Example : An email spam filter__\n",
    "\n",
    "> Solution using machine learning:\n",
    "- the user provides the algorithm with a large number of emails (input) and marks them as spam or not spam (desired output). \n",
    "- Given a new email, the algorithm predicts whether the new email is spam or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='SupervisedLearningAlgorithms'></a>\n",
    "## 1.2 Supervised Learning Algorithms\n",
    "\n",
    "\n",
    "\n",
    "Machine learning algorithms that learn from input/output pairs.\n",
    "\n",
    "\n",
    "A “teacher” provides supervision to the algorithms in the form of the desired outputs for each example that they learn from. \n",
    "\n",
    "While creating a dataset of inputs and outputs is often a laborious manual process, supervised learning algorithms are well understood and their performance is easy to measure. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Examples of __supervised__ machine learning tasks:\n",
    "- Identifying an address from handwriting on an envelope\n",
    "    - Input : scan of the handwriting\n",
    "    - Output : actual digits in the zip code\n",
    "    - Training dataset (for building a machine learning model) : many images of envelopes with addresses input digitally.  \n",
    "\n",
    "<br>\n",
    "\n",
    "- Determining whether a tumor is benign based on a medical image\n",
    "    - Input : the image\n",
    "    - Output : whether the tumor is benign. \n",
    "    - Training dataset : database of medical images, each image tagged with expert diagnosis. \n",
    "\n",
    "<br>\n",
    "\n",
    "- Detecting fraudulent activity in credit card transactions\n",
    "    - Input : record of credit card transaction\n",
    "    - Output : likely to be fraudulent or not.\n",
    "    - Training dataset : recording all transactions and tag any where a user has reported fraudulant activity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='UnsupervisedLearningAlgorithms'></a>\n",
    "## 1.3 Unsupervised Learning Algorithms \n",
    "\n",
    "\n",
    "\n",
    "Only the input data is known.\n",
    "\n",
    "No known output data is given to the algorithm. \n",
    "\n",
    "There are many successful applications of these methods, but they are usually harder to understand and evaluate. \n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Examples of __unsupervised learning__:\n",
    "\n",
    "- Identifying topics in a set of blog posts : \n",
    "    - summarize a large collaection of text data and find prevalent themes. \n",
    "    - topics and number of topics may be unknown --> no known outputs.\n",
    "\n",
    "- Dividing customers by preference\n",
    "    - identify which customers are similar (e.g. “parents,” “students, “gamers”)\n",
    "    - group by similar preferences. \n",
    "    - groups, number of groups, size of groups --> no known outputs.\n",
    "\n",
    "- Detecting abnormal access patterns to a website\n",
    "     - find abnormal access patterns to identify abuse or bugs.\n",
    "     - each abnormal pattern may be different, there may be no records of abnormal behaviour --> no known outputs   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For both supervised and unsupervised learning tasks, it is important to have a representation of your input data that a computer can understand. \n",
    "\n",
    "It can be helpful to arrange your data as a table. \n",
    "\n",
    "__Rows__ : each data point(each email, each customer, each transaction)\n",
    "\n",
    "__Columns__ : properties of each data point (age of a customer or the amount/location of a transaction)\n",
    "\n",
    "Later we will go into more detail on the topic of building a good representation of your data (known as feature extraction or feature engineering)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Examples : \n",
    "\n",
    "Shop\n",
    "<br>Rows : customers \n",
    "<br>Columns : age, gender, purchased items\n",
    "\n",
    "Tumor \n",
    "<br>Rows : image of tumor\n",
    "<br>Columns : grayscale values of each pixel, size/shape/color of the tumor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In machine learning \n",
    "\n",
    "Rows are known as a samples (or data point)\n",
    "\n",
    "Columns are called features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "### Understanding Your Task and Understanding Your Data\n",
    "Quite possibly the most important part in the machine learning process is understanding:\n",
    "- the data you are working with\n",
    "- how it relates to the task you want to solve. \n",
    "\n",
    "It is a good idea to keep the bigger picture in mind at all times.\n",
    "\n",
    "This will avoid spending much time building a complex machine learning solution that does not solve the right problem.\n",
    "\n",
    "It is not an effective strategy to randomly choose an algorithm and run it on your data.\n",
    "\n",
    "Each algorithm is different in terms of what kind of data and what problem setting it works best for.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It is necessary to understand what is going on in your dataset before you begin building a model. \n",
    "\n",
    "- What question(s) are you trying to answer? Do I think the data collected can answer that question?\n",
    "\n",
    "- What is the best way to phrase the question(s) as a machine learning problem?\n",
    "\n",
    "- Have you collected enough data to represent the problem I want to solve?\n",
    "\n",
    "- What features of the data did I extract, and will these enable the right predictions?\n",
    "\n",
    "- How will I measure success in my application?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "<a id='MachineLearningPython'></a>\n",
    "# 2. Machine Learning with Python\n",
    "\n",
    "Large array of general-purpose and specialized functionality:\n",
    "<br>Libraries for data loading, visualization, statistics, natural language processing, image processing...\n",
    "\n",
    "Machine learning and data analysis are fundamentally iterative processes and require tools to iterate as fast as possible. \n",
    "\n",
    "Python also allows for the creation of complex graphical user interfaces (GUIs) and web services, and for integration into existing systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `scikit-learn`\n",
    "\n",
    "- contains a number of state-of-the-art machine learning algorithms + documentation about each algorithm.\n",
    "- open source project: constantly being developed and improved\n",
    "- active user community\n",
    "- most widely used Python library for machine learning. \n",
    "- widely used in industry and academia\n",
    "- works well with a number of other scientific Python tools.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='InstallPandas'></a>\n",
    "# Install  `scikit-learn` and `mglearn`\n",
    "\n",
    "\n",
    "\n",
    "##### Windows \n",
    "\n",
    "1. Open the Anaconda Prompt from the terminal.\n",
    "<p align=\"center\">\n",
    "  <img src=\"img/anaconda_prompt.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "</p>\n",
    "\n",
    "1. Run the following:  \n",
    ">`conda install -c intel scikit-learn`<br>\n",
    ">`conda install -c intel mglearn`<br>\n",
    ">`conda install python-graphviz`\n",
    "\n",
    "or alternatively:\n",
    "\n",
    ">`pip install -U scikit-learn`<br>\n",
    ">`pip install mglearn`<br>\n",
    ">`pip install python-graphviz`\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Mac\n",
    "\n",
    "1. Open a terminal. \n",
    "\n",
    "1. Type the following:  \n",
    ">`conda install -c intel scikit-learn`<br>\n",
    ">`conda install -c intel mglearn`<br>\n",
    ">`conda install python-graphviz`\n",
    "\n",
    "or alternatively:\n",
    "\n",
    ">`pip install -U scikit-learn`<br>\n",
    ">`pip install mglearn`<br>\n",
    ">`pip install python-graphviz`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check `scikit` learn and all the other packages you need are installed correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(\"Python version:\", sys.version)\n",
    "\n",
    "import pandas as pd\n",
    "print(\"pandas version:\", pd.__version__)\n",
    "\n",
    "import matplotlib\n",
    "print(\"matplotlib version:\", matplotlib.__version__)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "\n",
    "import scipy as sp\n",
    "print(\"SciPy version:\", sp.__version__)\n",
    "\n",
    "import IPython\n",
    "print(\"IPython version:\", IPython.__version__)\n",
    "\n",
    "import sklearn\n",
    "print(\"scikit-learn version:\", sklearn.__version__)\n",
    "\n",
    "import mglearn\n",
    "print(\"mglearn version:\", mglearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='FirstMachineLearningApplication'></a>\n",
    "# 3. Our First Machine Learning Application: Classifying Iris Species\n",
    "\n",
    "<br> <a href='#IrisData'>3.1 Iris Data</a>\n",
    "<br> <a href='#TrainingTestingData'>3.2 Training and Testing Data</a> \n",
    "<br> <a href='#ExamineData'>3.3 First Step: Examine the Data</a>\n",
    "<br> <a href='#BuildingYourFirstModel'>3.4 Building Your First Model: k-Nearest Neighbors</a>\n",
    "<br> <a href='#MakingPredictions'>3.5 Making Predictions</a>\n",
    "<br> <a href='#EvaluatingModel'>3.6 Evaluating the Model</a> \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We will go through a simple machine learning application and create our first model. \n",
    "\n",
    "In the process, we will introduce some core concepts and terms.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Goal : predict which species an iris belongs to using measurements:\n",
    "- length and width of sepal\n",
    "- length and width of petal\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"img/iris.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "</p>\n",
    "\n",
    "Training data :\n",
    "Measurements of some irises that have been previously identified by an expert as belonging to one of three species:\n",
    "- setosa\n",
    "- versicolor\n",
    "- virginica. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have measurements tagged with the correct species of iris --> __supervised learning problem__. \n",
    "<br>For a particular data point, the species it belongs to is called its __label__.\n",
    "\n",
    "We want to predict one of several options (the species of iris) --> __classification problem__. \n",
    "<br>The possible outputs (different species of irises) are called classes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='IrisData'></a>\n",
    "## 3.1 Iris Data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The Iris dataset we will use is a widely used example when studying machine learning and statistics. \n",
    "\n",
    "It is included in scikit-learn in the dataset module. \n",
    "\n",
    "We can load it by calling the load_iris function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris_dataset = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The iris object that is returned by load_iris is a `Bunch` object, which is very similar to a `dictionary` or `DataFrame`. \n",
    "\n",
    "It contains keys and values:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Keys of iris_dataset:\\n\", iris_dataset.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`target_names` is an array of strings: the species of flower that we want to predict:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Target names:\", iris_dataset['target_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`feature_names` is a list of strings, giving the description of each feature:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature names:\\n\", iris_dataset['feature_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`target` and `data` contains the numeric data itself : measurements of sepal length, sepal width, petal length, and petal width (cm) in a NumPy array\n",
    "\n",
    "Rows (samples): flowers\n",
    "\n",
    "Columns (features): four measurements taken for each flower:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Type of data:\", type(iris_dataset['data']))\n",
    "\n",
    "print(\"First five rows of data:\\n\", iris_dataset['data'][:5,:])\n",
    "\n",
    "print(\"Shape of data:\", iris_dataset['data'].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We see that the array contains measurements for 150 different flowers.\n",
    "\n",
    "The shape of the data array is the number of samples times the number of features.\n",
    "\n",
    "All of the first five flowers have a petal width of 0.2 cm\n",
    "<br>The first flower has the longest sepal, at 5.1 cm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`target` 1D NumPy array : species of each flower: \n",
    "\n",
    "Species are encoded as integers from 0 to 2:\n",
    "- 0 = setosa\n",
    "- 1 = versicolor\n",
    "- 2 = virginica\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Type of target:\", type(iris_dataset['target']))\n",
    "\n",
    "print(\"Shape of target:\", iris_dataset['target'].shape)\n",
    "\n",
    "print(\"Target:\\n\", iris_dataset['target'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`target names` : The meanings of the numbers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Species names:\", iris_dataset['target_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='TrainingTestingData'></a>\n",
    "## 3.2 Measuring Success: Training and Testing Data\n",
    "\n",
    " \n",
    "\n",
    "- build a machine learning model from training data\n",
    "- test that the model can predict the species of iris for a new set of measurements\n",
    "- apply our model to new measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We cannot use the data we used to build the model to evaluate it. \n",
    "\n",
    "This is because our model will remember the whole training set\n",
    "\n",
    "Therefore it will always predict the correct label for any point in the training set. \n",
    "\n",
    "This “remembering” does not indicate to us whether our model will generalize well (make accurate predictions on new data).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To assess the model’s performance, we instead show it *new* data for which we have labels. \n",
    "\n",
    "This is usually done by splitting the labeled data we have collected (here, our 150 flower measurements) into two parts:\n",
    "- training data or training set : used to build the machine learning model\n",
    "- test data, test set : the rest of the data is used to assess how well the model works\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`scikit-learn` has a function that shuffles the dataset and splits it for you: the `train_test_split` function:\n",
    "\n",
    "- training set : 75% of the rows in the data + corresponding labels for this data. \n",
    "- test set : remaining 25% of the data + remaining labels\n",
    "\n",
    "You can split the data in any ratio you like, but 75:25 is commonly used. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In `scikit-learn`:\n",
    "- data is usually denoted with a *capital* `X`\n",
    "- labels are denoted by a *lowercase* `y`\n",
    "\n",
    "This is inspired by the standard formulation $f(x)=y$ in mathematics, where:\n",
    "- $x$ = input to a function\n",
    "- $y$ = output \n",
    "\n",
    "Following more conventions from mathematics:\n",
    "- *capital* `X` : the data is a 2D array (a matrix)\n",
    "- *lowercase* `y` : target is a 1D array (a vector).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let’s call train_test_split on our data and assign the outputs using this nomenclature:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_dataset['data'], \n",
    "                                                    iris_dataset['target'], \n",
    "                                                    random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Before making the split, the `train_test_split` function shuffles the dataset using a pseudorandom number generator.\n",
    "\n",
    "`random_state` : an integer value that sets a seed to the random generator. \n",
    "<br>The train-test split is random but the same every time you run the code. \n",
    "<br>If you don't set a seed, it is different each time.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As the data points are sorted by the label (see the output for `iris['target']` shown earlier), if we just took the last 25% of the data as a test set, all the data points would have the label 2, . \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Using a test set containing only one of the three classes would not tell us much about how well our model generalizes, so we shuffle our data to make sure the test data contains data from all classes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`train_test_split` output : `X_train, X_test, y_train, and y_test`(NumPy arrays). \n",
    "\n",
    "`X_train` contains 75% of the rows of the dataset\n",
    "\n",
    "`X_test` contains the remaining 25%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='ExamineData'></a>\n",
    "## 3.3 First Step: Examine the Data\n",
    "\n",
    "\n",
    "\n",
    "Before building a machine learning model it is a good idea to inspect the data, to see if:\n",
    "- the task is easily solvable without machine learning\n",
    "- the desired information might not be contained in the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Inspecting your data is a good way to find abnormalities.\n",
    "\n",
    "Maybe some of the irises were measured using inches and not centimeters, for example. \n",
    "\n",
    "In the real world, inconsistencies in the data and unexpected measurements are very common.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualising Data\n",
    "\n",
    "We can visualize data by using a scatter plot. \n",
    "\n",
    "We are restricted to plotting only two (or maybe three) features at a time. \n",
    "\n",
    "It is difficult to plot datasets with more than three features this way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pair plot\n",
    "\n",
    "A solution to this problem : compares all possible __pairs__ of features. \n",
    "\n",
    "If you have a small number of features, such as the four we have in the iris data set, this is quite reasonable. \n",
    "\n",
    "__Note:__  a pair plot does not show the interaction of *all of features at once*.\n",
    "<br>Some interesting aspects of the data may not be revealed when visualizing it this way.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The following code creates a pair plot of the features in the training set. \n",
    "\n",
    "The data points are colored according to the species the iris belongs to. \n",
    "\n",
    "To create the plot, we first convert the NumPy array into a pandas DataFrame. \n",
    "\n",
    "pandas has a function to create pair plots called `scatter_matrix`. \n",
    "\n",
    "The diagonal of this matrix is filled with histograms of each feature, showing the distribution. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# create dataframe from data in X_train\n",
    "iris_dataframe = pd.DataFrame(X_train, \n",
    "                              columns=iris_dataset.feature_names) # label columns using iris_dataset.feature_names\n",
    "\n",
    "\n",
    "# create a scatter matrix from the dataframe, color by y_train\n",
    "pd.plotting.scatter_matrix(iris_dataframe,        # data frame\n",
    "                           c=y_train,             # colour by y_train\n",
    "                           figsize=(10, 10),\n",
    "                           marker='o', \n",
    "                           hist_kwds={'bins': 20},# plotting keyword arguments to be passed to hist function\n",
    "                           s=60,                  # size of markers\n",
    "                           alpha=.8,              # transparency of markers\n",
    "                           cmap='viridis');         # colour map used for colour of each data plotted\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"img/iris_scatter.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "</p>\n",
    "\n",
    "From the plots, we can see that the three classes seem to be relatively well separated using the sepal and petal measurements. \n",
    "\n",
    "This means that a machine learning model will likely be able to learn to separate them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='BuildingYourFirstModel'></a>\n",
    "## 3.4 Building Your First Model: k-Nearest Neighbors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Now we can start building the actual machine learning model. \n",
    "\n",
    "There are many classification algorithms in `scikit-learn` that we could use. \n",
    "\n",
    "Here we will use a k-nearest neighbors classifier, which should be easy to understand as we applied it as a hand-coded solution earlier in the class (athletes example). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__k-Nearest Neighbours__\n",
    "- Store training set. \n",
    "- To make a prediction for a new data point, find the point in the training set that is closest to the new point. \n",
    "- Assign the label of this training point to the new data point.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can consider any fixed number k of neighbors in the training (for example, the closest three or five neighbors). \n",
    "\n",
    "Then, we can make a prediction using the majority class among these neighbors. \n",
    "\n",
    "We will go into more detail about this later; for now, we’ll use just 1 neighbor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The k-nearest neighbors classification algorithm comes from the `KNeighborsClassifier` __class__.\n",
    "\n",
    "Before we can use the model, we need to *instantiate* the class into an __object__. \n",
    "\n",
    "This is when we will set any parameters of the model. \n",
    "\n",
    "The most important parameter of KNeighborsClassifier is the number of neighbors, which we will set to 1:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier # import model\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)          # instatiate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The `knn` object encapsulates:\n",
    "- algorithm that will be used to build the model from the training data\n",
    "- algorithm to make predictions on new data points. \n",
    "- information that the algorithm extracts from the training data. <br>(In the case of `KNeighborsClassifier` this is just the training data set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To build the model on the training set, we call the `fit` method of the `knn` object.\n",
    "\n",
    "Arguments:\n",
    "- training data : the NumPy array `X_train`\n",
    "- corresponding training labels : NumPy array `y_train`\n",
    "\n",
    "The model is now *fitted* to the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The `fit` method returns the `knn` object itself (and modifies it in place).\n",
    "<br>The output is a string representation of the classifier. \n",
    "\n",
    "The representation shows us which parameters were used in creating the model. \n",
    "- Nearly all of them are the default values.\n",
    "- `n_neighbors=1` is the parameter that we set. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Most models in `scikit-learn` have many optional parameters that you can set to customise the algorithm.\n",
    "<br>Printing a `scikit-learn` model can return very long strings.\n",
    "\n",
    "In this example, we will just use the default values.  \n",
    "<br>We will study how to set important parameters in next week's class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='MakingPredictions'></a>\n",
    "## Making Predictions\n",
    "We can now make predictions by applying the fitted model to new data for which we might not know the correct labels. \n",
    "\n",
    "Imagine we found an iris in the wild with:\n",
    "- sepal length = 5 cm\n",
    "- sepal width = 2.9 cm\n",
    "- petal length = 1 cm\n",
    "- petal width = 0.2 cm\n",
    "\n",
    "What species of iris would this be? \n",
    "\n",
    "We can put this data into a NumPy array...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_new = np.array([[5, 2.9, 1, 0.2]])\n",
    "print(\"X_new.shape:\", X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we made the measurements of this single flower into a row in a two-dimensional NumPy array:\n",
    "```\n",
    "([[...]])\n",
    "```\n",
    "as scikit-learn always expects *two-dimensional arrays* for the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To make a prediction, we call the `predict` method of the `knn` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = knn.predict(X_new) # make prediction about new data\n",
    "\n",
    "print(f\"Prediction: {prediction}\")\n",
    "\n",
    "print(f\"Predicted target name: {iris_dataset['target_names'][prediction]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our model predicts that the new iris belongs to:\n",
    "- the class 0\n",
    "- the species setosa\n",
    "\n",
    "But how do we know if we can trust our model? \n",
    "\n",
    "We don’t know the *correct* species of this sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='EvaluatingModel'></a>\n",
    "## 3.6 Evaluating the Model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We can use the __test__ set that we created earlier.\n",
    "\n",
    "This data *was not* used to build the model.\n",
    "<br>BUT we *do* know what the correct species is for each iris in the test data set.\n",
    "\n",
    "Therefore, we can make a prediction for each iris in the test data and compare it against its label (the known species). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can measure how well the model works by computing the accuracy: <br>the fraction of flowers for which the right species was predicted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print(f\"Test set predictions:\\n{y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test set score: {np.round(np.sum(y_pred == y_test)/len(y_test), 3)}\")\n",
    "\n",
    "#print(f\"Test set score: {np.round(np.mean(y_pred == y_test), 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can also use the `score` method of the `knn` object, which will compute the test set accuracy for us:\n",
    "\n",
    "`score` uses a default evaluation criterion for each estimator, detailed in each estimator’s documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test set score: {knn.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For this model, the test set accuracy is about 0.97.\n",
    "\n",
    "The model made the right prediction for 97% of the irises in the test set. \n",
    "\n",
    "Under some mathematical assumptions, this means that we can expect our model to be correct 97% of the time for new irises. \n",
    "\n",
    "For our application, this high level of accuracy means that our model may be reliable enough. \n",
    "\n",
    "Next week we will discuss how we can improve performance, and tune a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='ImportingData'></a>\n",
    "# 4. Importing Data\n",
    "\n",
    "\n",
    "The iris data set is a useful \"toy\" data set. \n",
    "\n",
    "However, in most cases we want to use real data, imported from external sources. \n",
    "\n",
    "`scikit_learn` works well with the pandas framework and we can run it's algorithms directly on Pandas `DataFrame`s. \n",
    "\n",
    "Let's take an example `DataFrame` from last week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We are importing the `sample_student_data` set, selecting only the `Sex`, `Height` and `Weight` columns.\n",
    "\n",
    "Let's examine the data to see if we can use the `Height` and `Weight` to accurately predict the `Sex` of a student. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "students = pd.read_csv('sample_data/sample_student_data.csv', skiprows=[1])\n",
    "\n",
    "students = students.loc[: , ['Sex', 'Height', 'Weight']]\n",
    "\n",
    "students.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The first thing we need to do is make the `Sex` column into numerical data so that it can be understood by the model. \n",
    "\n",
    "This is a common operation required for processing of real data. \n",
    "\n",
    "Later, we will study functions to achieve this using more complex data. \n",
    "\n",
    "For now, we will use a simple boolean array to convert:\n",
    "- `M` --> `0`\n",
    "- `F` --> `1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A simple boolean array works will for *binary* data (only 2 classes). \n",
    "\n",
    "If we want to assign a unique integer value to a set of >2 classes, we need to do a little bit more work. \n",
    "\n",
    "We will introduce ways to automate this process later. \n",
    "\n",
    "For now, we will use some famiilar processes to assign the integer values: `list` and `for` loop..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students['Sex'] = students['Sex'] == 'M'       # boolean array using comparison operator\n",
    "\n",
    "students['Sex'] = students['Sex'].astype(int)  # integer array using type conversion\n",
    "\n",
    "students.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# import data\n",
    "students = pd.read_csv('sample_data/sample_student_data.csv', skiprows=[1])  \n",
    "students = students.loc[: , ['Sex', 'Height', 'Weight']]\n",
    "\n",
    "\n",
    "# a list of unique string values\n",
    "sexes = list(students['Sex'].unique())\n",
    "#sexes = list(students.Sex.unique())\n",
    "print(sexes)\n",
    "\n",
    "# loop through each item in list and replace string item with a string number in DataFrame\n",
    "for n, m in enumerate(sexes):\n",
    "    print(n,m)\n",
    "    students = students.replace({m: n})\n",
    "    #students['Sex'] = students['Sex'].str.replace(m, str(n), regex=True)\n",
    "\n",
    "# # convert string numerical data to integer\n",
    "students['Sex'] = students['Sex'].astype(int)\n",
    "\n",
    "# # display result\n",
    "students.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's perform some of the operations we performed on the `iris` data set.\n",
    "\n",
    "First we will split the data into test and training data sets. \n",
    "\n",
    "We can input the features and labels of the full data set as Pandas `Series` and `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(students.loc[:,'Height' :], # features\n",
    "                                                    students['Sex'],            # labels\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Examine the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A scatter plot is created to compare the features and look for correlations.\n",
    "\n",
    "Colour maps to produce the plot:\n",
    "https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scatter matrix from the X_train dataframe, color by y_train\n",
    "\n",
    "pd.plotting.scatter_matrix(X_train,                 # data frame\n",
    "                           c=y_train,               # colour by y_train\n",
    "                           figsize=(10, 10),\n",
    "                           marker='o', \n",
    "                           hist_kwds={'bins': 20},  # plotting keyword arguments to be passed to hist function\n",
    "                           s=60,                    # size of markers\n",
    "                           alpha=.8,                # transparency of markers\n",
    "                           cmap='viridis');         # colour map used for colour of each data plotted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "From the plots, we can see that the two classes:  females (purple) and males (yellow) seem to be relatively well separated using height and weight. \n",
    "\n",
    "This means that a machine learning model will likely be able to learn to separate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So all that's left to do is train the model and see how accuarately it predicts the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Test set score: {np.round(knn.score(X_test, y_test), 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "0.667 isn't a great test score, but our data set is very small so the accuracy is reasonable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "TODO : Add section on PCA\n",
    "\n",
    "References : \n",
    "\n",
    "https://www.dezyre.com/data-science-in-python-tutorial/principal-component-analysis-tutorial\n",
    "\n",
    "https://georgemdallas.wordpress.com/2013/10/30/principal-component-analysis-4-dummies-eigenvectors-eigenvalues-and-dimension-reduction/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First scale the data (exalpantion in next notebook on scaling data)\n",
    "\n",
    "PCA performs best with a normalized feature set. \n",
    "\n",
    "We will perform standard scalar normalization to normalize our feature set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler() \n",
    "\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing PCA using Scikit-Learn is a two-step process:\n",
    "\n",
    "1. Initialize the PCA class by passing the number of components to the constructor.\n",
    "1. Call the fit and then transform methods by passing the feature set to these methods. The transform method returns the specified number of principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance caused by each of the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance = pca.explained_variance_ratio_\n",
    "explained_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicitly specify how much variance we would like PCA to capture. \n",
    "\n",
    "The n_components will vary based on the variance parameter.\n",
    "\n",
    "(If you do not pass any variance, then the number of components will be equal to the original dimension of the data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(0.9)      # Two components make up over 0.9 of the variance. --> n_components == 2\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A much higher score is achieved by removing the redundant dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Test set score: {np.round(knn.score(X_test, y_test), 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary \n",
    "- __Supervised learning__ : The user provides the algorithm with pairs of inputs and desired outputs.\n",
    "<br>__Un-supervised learning__ : Only the input data is known. No known output data is given to the algorithm.\n",
    "\n",
    "- Supervised Learning Algorithm\n",
    "    - build a machine learning model from training data\n",
    "    - test that the model can predict the species of iris for a new set of measurements\n",
    "    - apply our model to new measurements\n",
    "\n",
    "- Before building a machine learning model inspect the data, to see if:\n",
    "    - the task is easily solvable without machine learning\n",
    "    - the desired information might not be contained in the data\n",
    "\n",
    "- Evaluate the accuracuy of the fitted model using the test data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Summary of the code__ needed for the whole training and evaluation procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier    # 1. Import model\n",
    "from sklearn.datasets import load_iris                # 2. Import data\n",
    "iris_dataset = load_iris()\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_dataset['data'],     # 3. Split the data\n",
    "                                                    iris_dataset['target'], \n",
    "                                                    random_state=0)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1) # 4. Instantaite the model including any model parameters\n",
    "\n",
    "knn.fit(X_train, y_train)                 # 5. Fit the model to the training data\n",
    "\n",
    "score = knn.score(X_test, y_test)         # 6. Evaluate the accuracy of the model on the test data\n",
    "print(f\"Test set score: {np.round(score), 3}\") \n",
    "\n",
    "knn.predict(X_test)                       # 7. Predict the targets of the test data\n",
    "\n",
    "X_new = np.array([[5, 2.9, 1, 0.2]])      # 8. Predict the target of a new data point \n",
    "prediction = knn.predict(X_new)\n",
    "print(f\"Predicted target name: {iris_dataset['target_names'][prediction]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## METHOD CHAINING\n",
    "Notice that we can chain methids together:\n",
    "\n",
    "```\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn = knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "```\n",
    "\n",
    "...can be written as...\n",
    "\n",
    "```\n",
    "knn = KNeighborsClassifier(n_neighbors=1).fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "```\n",
    "\n",
    "...can be written as...\n",
    "```\n",
    "y_pred = KNeighborsClassifier(n_neighbors=1).fit(X_train, y_train).predict(X_test)\n",
    "```\n",
    "\n",
    "The method we choose depends on if we want to store the *fitted* KNN model as a variable.\n",
    "\n",
    "Storing it allows us to:\n",
    "- inspect it\n",
    "- use it to predict on other data\n",
    "\n",
    "Refitting the model to accomplish these tasks can take a considerable amount of time whent he data set it large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='ReviewExercises'></a>\n",
    "# 5. Review Exercise : KNN Classifier with Real Data\n",
    "\n",
    "You are now going to build a KNN classifier for the data in the `planets DataFrame` that we studied last week. \n",
    "\n",
    "The exercise will involve:\n",
    "- manipulating the data using:\n",
    "    - programing fundamentals\n",
    "    - Pandas operation studied last week\n",
    "- Building a KNN machine learning model to identify the __measurement method__ (`method`) used for a planet from feature data including it's orbital period and distance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1. Import the `planets` data from the `seaborn` package to a Pandas `DataFrame` as we did last week. \n",
    "1. Drop null (`NaN`) values:\n",
    "    - show how many null values appear in each column\n",
    "    - drop any *columns* where less than half the values are non-null\n",
    "    - drop all remaining *rows* containing null values\n",
    "1. Create a column with a __unique integer value__ to represent each unique string value in the `method` column of the `DataFrame`. <br>*Hint : If using the method demonstrated today, you must __reverse__ the order of the list created using the `.unique()` method. This is because the string `Transit` appears __twice__, once by itself and once as part of another string.*\n",
    "1. Split the data set into training and test data\n",
    "1. Create a scatter plot to check if the different `method` classes/targets can be separated using the features\n",
    "1. Import the KNN model, instantiate and fit the model to the training data\n",
    "1. What percentage of the test data does the model predict correctly? \n",
    "1. Look at step 5 again. <br>Do some features seperate the classes better than others? <br>What happens if you remove the features that do not seperate the classes well? <br>How does this effect the accuracy of the model prediction? <br>__Note__: Normally this process of identifying __feature importance__ would be automated. We will consider this in a later class. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revew Exercise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
